# This workflow will install Python dependencies, run tests and lint with a single version of Python
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python

name: arXiv-daily-ai-enhanced

on:
  schedule:
    - cron: "40 1 * * *"
  workflow_dispatch:

env:
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
  OPENAI_BASE_URL: ${{ secrets.OPENAI_BASE_URL }}
  PASSWORD: ${{ secrets.ACCESS_PASSWORD }}
  NAME: ${{ vars.NAME }}
  EMAIL: ${{ vars.EMAIL }}
  MODEL_NAME: ${{ vars.MODEL_NAME }}
  CATEGORIES: ${{ vars.CATEGORIES }}
  MAX_WORKERS: ${{ vars.MAX_WORKERS || '1' }}
  LANGUAGE: ${{ vars.LANGUAGE || 'Chinese' }}
  DEFAULT_KEYWORDS: ${{ vars.DEFAULT_KEYWORDS || ''}}
  DEFAULT_AUTHORS: ${{ vars.DEFAULT_AUTHORS || '' }}

jobs:
  build:

    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Install dependencies
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        uv sync
        
    - name: Crawl arXiv papers
      id: crawl_step
      run: |
        source .venv/bin/activate
        today=$(date -u "+%Y-%m-%d")
        echo "å¼€å§‹çˆ¬å– $today çš„arXivè®ºæ–‡... / Starting to crawl $today arXiv papers..."
        echo "çˆ¬å–ç±»åˆ«: $CATEGORIES / Crawling categories: $CATEGORIES"
        
        # æ£€æŸ¥ä»Šæ—¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚å­˜åœ¨åˆ™åˆ é™¤ / Check if today's file exists, delete if found
        if [ -f "data/crawler-data/${today}.jsonl" ]; then
            echo "ğŸ—‘ï¸ å‘ç°ä»Šæ—¥æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ­£åœ¨åˆ é™¤é‡æ–°ç”Ÿæˆ... / Found existing today's file, deleting for fresh start..."
            rm "data/crawler-data/${today}.jsonl"
            echo "âœ… å·²åˆ é™¤ç°æœ‰æ–‡ä»¶ï¼šdata/crawler-data/${today}.jsonl / Deleted existing file: data/crawler-data/${today}.jsonl"
        else
            echo "ğŸ“ ä»Šæ—¥æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå‡†å¤‡æ–°å»º... / Today's file doesn't exist, ready to create new one..."
        fi
        
        cd daily_arxiv

        # ä½¿ç”¨Scrapyçˆ¬å–
        # Use Scrapy to crawl
        scrapy crawl arxiv -o ../data/crawler-data/${today}.jsonl
        
        # æ£€æŸ¥çˆ¬å–æ˜¯å¦æˆåŠŸ / Check if crawling was successful
        if [ ! -f "../data/crawler-data/${today}.jsonl" ]; then
            echo "çˆ¬å–å¤±è´¥ï¼Œæœªç”Ÿæˆæ•°æ®æ–‡ä»¶ / Crawling failed, no data file generated"
            exit 1
        fi
        
        echo "crawl_date=$today" >> $GITHUB_OUTPUT
        echo "çˆ¬å–å®Œæˆ / Crawling completed"
        
    - name: Check for duplicates
      id: dedup_check
      run: |
        source .venv/bin/activate
        echo "æ‰§è¡Œå»é‡æ£€æŸ¥... / Performing intelligent deduplication check..."
        
        cd daily_arxiv
        # æ‰§è¡Œå»é‡æ£€æŸ¥è„šæœ¬ / Execute intelligent deduplication check script
        set +e  # æš‚æ—¶å…è®¸å‘½ä»¤å¤±è´¥ / Temporarily allow command failure
        python daily_arxiv/check_stats.py
        
        # è·å–é€€å‡ºç  / Get exit code
        dedup_exit_code=$?
        set -e  # æ¢å¤ä¸¥æ ¼æ¨¡å¼ / Restore strict mode
        
        echo "å»é‡æ£€æŸ¥é€€å‡ºç : $dedup_exit_code / Dedup check exit code: $dedup_exit_code"
        echo "dedup_exit_code=$dedup_exit_code" >> $GITHUB_OUTPUT
        
        case $dedup_exit_code in
            0)
                echo "has_new_content=true" >> $GITHUB_OUTPUT
                ;;
            1)
                echo "has_new_content=false" >> $GITHUB_OUTPUT
                echo "skip_reason=no_new_content" >> $GITHUB_OUTPUT
                ;;
            2)
                echo "has_new_content=false" >> $GITHUB_OUTPUT
                echo "skip_reason=processing_error" >> $GITHUB_OUTPUT
                ;;
            *)
                echo "âŒ æœªçŸ¥é€€å‡ºç ï¼Œåœæ­¢å·¥ä½œæµ / Unknown exit code, stop workflow"
                echo "has_new_content=false" >> $GITHUB_OUTPUT
                echo "skip_reason=unknown_error" >> $GITHUB_OUTPUT
                ;;
        esac
        
    - name: AI Enhancement Cache
      if: steps.dedup_check.outputs.has_new_content == 'true'
      uses: actions/cache@v4
      with:
        path: data/crawler-data/${{ steps.crawl_step.outputs.crawl_date }}_AI_enhanced_${{ env.LANGUAGE }}.jsonl
        key: ai-enhance-cache-${{ steps.crawl_step.outputs.crawl_date }}-${{ github.run_id }}
        restore-keys: |
          ai-enhance-cache-${{ steps.crawl_step.outputs.crawl_date }}-

    - name: AI Enhancement Processing
      id: ai_enhance
      if: steps.dedup_check.outputs.has_new_content == 'true'
      run: |
        source .venv/bin/activate
        today=${{ steps.crawl_step.outputs.crawl_date }}
        echo "å¼€å§‹AIå¢å¼ºå¤„ç†... / Starting AI enhancement processing..."
        
        cd ai
        echo "ä½¿ç”¨æ¨¡å‹: $MODEL_NAME / Using model: $MODEL_NAME"
        echo "è¾“å‡ºè¯­è¨€: $LANGUAGE / Output language: $LANGUAGE"
        echo "å¹¶è¡Œå¤„ç†å·¥ä½œçº¿ç¨‹æ•°: $MAX_WORKERS / Number of parallel worker threads: $MAX_WORKERS"
        
        # ä½¿ç”¨AIå¤„ç†çˆ¬å–çš„æ•°æ® / Use AI to process the crawled data
        LOG_FILE=$(mktemp)
        python enhance.py --data ../data/crawler-data/${today}.jsonl --max_workers $MAX_WORKERS 2>&1 | tee $LOG_FILE
        if tail -10 "$LOG_FILE" | grep -q "AI_FAILED"; then
            echo "âŒ AIå¤„ç†å­˜åœ¨é”™è¯¯ / AI processing has errors"
        else
            echo "ai_success=true" >> $GITHUB_OUTPUT
            echo "AIå¢å¼ºå¤„ç†å®Œæˆ / AI enhancement processing completed"
        fi
        rm "$LOG_FILE"

    - name: Update file list
      if: steps.ai_enhance.outputs.ai_success == 'true'
      run: |
        echo "æ›´æ–°æ–‡ä»¶åˆ—è¡¨... / Updating file list..."
        ls data/crawler-data/*.jsonl | sed 's|data/crawler-data/||' > data/metadata/file-list.txt
        echo "æ–‡ä»¶åˆ—è¡¨æ›´æ–°å®Œæˆ / File list updated"
        
    # - name: Convert to Markdown
    #   id: convert_md
    #   if: steps.ai_enhance.outputs.ai_success == 'true'
    #   run: |
    #     source .venv/bin/activate
    #     today=${{ steps.crawl_step.outputs.crawl_date }}
    #     echo "è½¬æ¢ä¸ºMarkdownæ ¼å¼... / Converting to Markdown format..."
        
    #     cd to_md
        
    #     # ä½¿ç”¨AIå¢å¼ºæ–‡ä»¶è¿›è¡Œè½¬æ¢ / Use AI enhanced file for conversion
    #     AI_FILE="../data/crawler-data/${today}_AI_enhanced_${LANGUAGE}.jsonl"
        
    #     if [ -f "$AI_FILE" ]; then
    #         echo "ä½¿ç”¨AIå¢å¼ºæ–‡ä»¶è¿›è¡Œè½¬æ¢... / Using AI enhanced file for conversion..."
    #         python convert.py --data "$AI_FILE"

    #         # æ£€æŸ¥è½¬æ¢æ˜¯å¦æˆåŠŸ / Check if conversion was successful
    #         if [ $? -ne 0 ]; then
    #             echo "Markdownè½¬æ¢å¤±è´¥ / Markdown conversion failed"
    #         else
    #             echo "convert_success=true" >> $GITHUB_OUTPUT
    #             echo "Markdownè½¬æ¢å®Œæˆ / Markdown conversion completed"
    #         fi
    #     else
    #         echo "é”™è¯¯ï¼šæœªæ‰¾åˆ°AIå¢å¼ºæ–‡ä»¶ / Error: AI enhanced file not found"
    #         echo "AIæ–‡ä»¶: $AI_FILE"
    #     fi

    - name: Summary
      run: |
        if [ "${{ steps.dedup_check.outputs.has_new_content }}" = "true" ]; then
          echo "âœ… å·¥ä½œæµå®Œæˆï¼šå»é‡å‘ç°æ–°å†…å®¹å¹¶æˆåŠŸå¤„ç† / Workflow completed: Smart deduplication found new content and processed successfully"
        else
          case "${{ steps.dedup_check.outputs.skip_reason }}" in
            "no_new_content")
              echo "â„¹ï¸ å·¥ä½œæµå®Œæˆï¼šå»é‡åæ— æ–°å†…å®¹ / Workflow completed: No new content after smart deduplication"
              ;;
            "processing_error")
              echo "âš ï¸ å·¥ä½œæµå®Œæˆï¼šå»é‡å¤„ç†å‡ºé”™ / Workflow completed: Deduplication processing error"
              ;;
            "unknown_error")
              echo "âš ï¸ å·¥ä½œæµå®Œæˆï¼šæœªçŸ¥é”™è¯¯ / Workflow completed: Unknown error"
              ;;
            *)
              echo "â„¹ï¸ å·¥ä½œæµå®Œæˆï¼šæœªçŸ¥åŸå› è·³è¿‡å¤„ç† / Workflow completed: Skipped for unknown reason"
              ;;
          esac
        fi

    - name: Generate password hash and inject into config
      if: steps.dedup_check.outputs.has_new_content == 'true'
      run: |
        echo "ğŸ” Generating password hash for authentication..."

        if [ -z "$PASSWORD" ]; then
          echo "âš ï¸  WARNING: ACCESS_PASSWORD not set in Secrets"
          echo "âš ï¸  Password protection will be DISABLED"
          echo "âš ï¸  To enable password protection, add ACCESS_PASSWORD in repository Secrets"
          echo "â„¹ï¸  Website will remain publicly accessible without authentication"

          # Use a special value that will disable authentication
          PASSWORD_HASH="DISABLED_NO_PASSWORD_SET_IN_SECRETS"
        else
          # Generate SHA-256 hash using openssl
          PASSWORD_HASH=$(echo -n "$PASSWORD" | openssl dgst -sha256 -hex | awk '{print $2}')
          echo "âœ… Password hash generated successfully"
          echo "âœ… Hash length: ${#PASSWORD_HASH} characters"
          echo "ğŸ” Password protection is ENABLED"
        fi

        # Inject hash into auth-config.js
        if [ -f "js/auth-config.js" ]; then
          sed -i "s/PLACEHOLDER_PASSWORD_HASH/$PASSWORD_HASH/" js/auth-config.js
          echo "âœ… Password hash injected into js/auth-config.js"
        else
          echo "âŒ ERROR: js/auth-config.js not found!"
        fi

        echo "ğŸ” Authentication setup complete"

    - name: Get default filter keywords & authors and inject into config
      if: steps.dedup_check.outputs.has_new_content == 'true'
      run: |
        echo "ğŸ” Get default filter keywords & authors..."

        # Inject hash into settings.js
        if [ -f "js/settings.js" ]; then
          sed -i "s/PLACEHOLDER_DEFAULT_KEYWORDS/$DEFAULT_KEYWORDS/" js/settings.js
          sed -i "s/PLACEHOLDER_DEFAULT_AUTHORS/$DEFAULT_AUTHORS/" js/settings.js
          echo "âœ… Default keywords and authors injected into js/settings.js"
        else
          echo "âŒ ERROR: js/settings.js not found!"
        fi

        echo "ğŸ” Filter keywords and authors setup complete"
        
    - name: Commit changes
      if: steps.ai_enhance.outputs.ai_success == 'true'
      run: |
        git config --global user.email "$EMAIL"
        git config --global user.name "$NAME"
        git add .
        # æ£€æŸ¥æ˜¯å¦æœ‰å˜æ›´éœ€è¦æäº¤ / Check if there are changes to commit
        if git diff --staged --quiet; then
          echo "ğŸŸ¡ æ²¡æœ‰å˜æ›´éœ€è¦æäº¤ / No changes to commit"
          exit 0
        fi
        git commit -m "update: $(date -u '+%Y-%m-%d') arXiv papers"
        echo "âœ… å˜æ›´å·²æäº¤ / Changes committed"
        
    - name: Pull latest changes and push
      if: steps.ai_enhance.outputs.ai_success == 'true'
      run: |
        # è®¾ç½®Gité…ç½®ä»¥å¤„ç†è‡ªåŠ¨åˆå¹¶ / Set Git config for automatic merging
        git config pull.rebase true
        git config rebase.autoStash true
        
        # å°è¯•æ¨é€ï¼Œå¦‚æœå¤±è´¥åˆ™æ‹‰å–å¹¶é‡è¯• / Try to push, if failed then pull and retry
        for i in {1..3}; do
          echo "æ¨é€å°è¯• $i / Push attempt $i"
          if git push origin main; then
            echo "âœ… æ¨é€æˆåŠŸ / Push successful"
            break
          else
            echo "ğŸŸ¡ æ¨é€å¤±è´¥ï¼Œæ‹‰å–æœ€æ–°å˜æ›´... / Push failed, pulling latest changes..."
            git pull origin main --no-edit || true
            if [ $i -eq 3 ]; then
              echo "âŒ 3æ¬¡å°è¯•åæ¨é€å¤±è´¥ / Failed to push after 3 attempts"
            fi
          fi
        done
